{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0beb38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "administration_prefix_patterns = {\n",
    "    \"province\": [\n",
    "        r\"^(thành phố|t\\.\\s*phố|tỉnh|tp|tp\\.|t\\.)\\s+\",\n",
    "        r\"^(thanh pho|tinh)\\s+\",  # no diacritics\n",
    "    ],\n",
    "    \"district\": [\n",
    "        r\"^(huyện|quận|thị xã|thành phố|q\\.|h\\.|tx\\.|tp\\.)\\s+\",\n",
    "        r\"^(huyen|quan|thi xa|thanh pho)\\s+\",  # no diacritics\n",
    "    ],\n",
    "    \"ward\": [\n",
    "        r\"^(phường|xã|thị trấn|p\\.|x\\.|tt\\.)\\s+\",\n",
    "        r\"^(phuong|xa|thi tran)\\s+\",  # no diacritics\n",
    "    ],\n",
    "}\n",
    "\n",
    "street_keywords = [\n",
    "    \"số\",\n",
    "    \"đường\",\n",
    "    \"bis\",\n",
    "    \"ấp\",\n",
    "    \"khu\",\n",
    "    \"block\",\n",
    "    \"lô\",\n",
    "    \"tổ\",\n",
    "    \"ngõ\",\n",
    "    \"hẻm\",\n",
    "    \"phố\",\n",
    "    \"street\",\n",
    "    \"road\",\n",
    "    \"avenue\",\n",
    "    \"lane\",\n",
    "    \"area\",\n",
    "    \"building\",\n",
    "    \"floor\",\n",
    "    \"apartment\",\n",
    "    \"căn hộ\",\n",
    "    \"tầng\",\n",
    "    \"toà\",\n",
    "    \"tòa\",\n",
    "    \"nhà\",\n",
    "    \"villa\",\n",
    "    \"biệt thự\",\n",
    "    \"chung cư\",\n",
    "    \"kdc\",\n",
    "    \"ktx\",\n",
    "    \"ccx\",\n",
    "    \"c/c\",\n",
    "    \"đs\",\n",
    "    \"km\",\n",
    "    \"ql\",\n",
    "    \"tl\",\n",
    "]\n",
    "\n",
    "common_province_alias_map = {\n",
    "    \"hcm\": \"hồ chí minh\",\n",
    "    \"hn\": \"hà nội\",\n",
    "    \"hnoi\": \"hà nội\",\n",
    "    \"t.t.h\": \"thừa thiên huế\",\n",
    "}\n",
    "\n",
    "# Single letter abbreviations that could be prefixes\n",
    "SINGLE_LETTER_PREFIXES = {\"x\", \"h\", \"q\", \"t\", \"p\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fbc5b1f",
   "metadata": {},
   "source": [
    "# Build BK-tree for fast prefix matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2488dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Tuple, List\n",
    "\n",
    "\n",
    "def levenshtein_distance(s1: str, s2: str) -> int:\n",
    "    \"\"\"Calculate Levenshtein distance between two strings\"\"\"\n",
    "    if len(s1) < len(s2):\n",
    "        return levenshtein_distance(s2, s1)\n",
    "\n",
    "    if len(s2) == 0:\n",
    "        return len(s1)\n",
    "\n",
    "    previous_row = list(range(len(s2) + 1))\n",
    "    for i, c1 in enumerate(s1):\n",
    "        current_row = [i + 1]\n",
    "        for j, c2 in enumerate(s2):\n",
    "            insertions = previous_row[j + 1] + 1\n",
    "            deletions = current_row[j] + 1\n",
    "            substitutions = previous_row[j] + (c1 != c2)\n",
    "            current_row.append(min(insertions, deletions, substitutions))\n",
    "        previous_row = current_row\n",
    "\n",
    "    return previous_row[-1]\n",
    "\n",
    "\n",
    "def normalize_for_comparison(text: str) -> str:\n",
    "    \"\"\"Normalize text for fuzzy matching by converting to lowercase\"\"\"\n",
    "    return text.lower().strip()\n",
    "\n",
    "\n",
    "class BKTreeNode:\n",
    "    \"\"\"Node for BK-Trie (Burkhard-Keller Trie) structure\"\"\"\n",
    "\n",
    "    def __init__(self, word: str = None):\n",
    "        self.word = word\n",
    "        self.normalized_word = normalize_for_comparison(word) if word else None\n",
    "        self.children = {}  # distance -> [BKTreeNode]\n",
    "        self.is_terminal = word is not None\n",
    "\n",
    "    def add_word(self, word: str):\n",
    "        \"\"\"Add a word to the BK-Trie\"\"\"\n",
    "        if self.word is None:\n",
    "            # This is root node, make it the first word\n",
    "            self.word = word\n",
    "            self.normalized_word = normalize_for_comparison(word)\n",
    "            self.is_terminal = True\n",
    "            return\n",
    "\n",
    "        # Calculate distance from current node's word\n",
    "        distance = levenshtein_distance(\n",
    "            self.normalized_word, normalize_for_comparison(word)\n",
    "        )\n",
    "\n",
    "        if distance in self.children:\n",
    "            # Find appropriate child to recurse into\n",
    "            added = False\n",
    "            for child in self.children[distance]:\n",
    "                if child.word == word:\n",
    "                    # Word already exists\n",
    "                    return\n",
    "                # Try to add to this child\n",
    "                child.add_word(word)\n",
    "                added = True\n",
    "                break\n",
    "            if not added:\n",
    "                # Create new child at this distance\n",
    "                self.children[distance].append(BKTreeNode(word))\n",
    "        else:\n",
    "            # Create new distance bucket\n",
    "            self.children[distance] = [BKTreeNode(word)]\n",
    "\n",
    "    def search(self, query: str, max_distance: int = 2) -> List[Tuple[str, int]]:\n",
    "        \"\"\"Search for words within max_distance of query\"\"\"\n",
    "        results = []\n",
    "        normalized_query = normalize_for_comparison(query)\n",
    "\n",
    "        if self.word is not None:\n",
    "            distance = levenshtein_distance(self.normalized_word, normalized_query)\n",
    "            if distance <= max_distance:\n",
    "                results.append((self.word, distance))\n",
    "\n",
    "        # Search children within distance range\n",
    "        for child_distance, child_nodes in self.children.items():\n",
    "            # Only search children if they could contain matches\n",
    "            if abs(child_distance - distance) <= max_distance:\n",
    "                for child in child_nodes:\n",
    "                    results.extend(child.search(query, max_distance))\n",
    "\n",
    "        return results\n",
    "\n",
    "\n",
    "class BKTree:\n",
    "    \"\"\"BK-Trie for efficient fuzzy string matching\"\"\"\n",
    "\n",
    "    def __init__(self, words: List[str] = None):\n",
    "        self.root = BKTreeNode()\n",
    "        self.word_count = 0\n",
    "\n",
    "        if words:\n",
    "            for word in words:\n",
    "                self.add_word(word)\n",
    "\n",
    "    def add_word(self, word: str):\n",
    "        \"\"\"Add a word to the trie\"\"\"\n",
    "        if word and word.strip():\n",
    "            self.root.add_word(word.strip())\n",
    "            self.word_count += 1\n",
    "\n",
    "    def search(self, query: str, max_distance: int = 2) -> List[Tuple[str, int]]:\n",
    "        \"\"\"Search for words within max_distance of query\"\"\"\n",
    "        if not query or not query.strip():\n",
    "            return []\n",
    "\n",
    "        results = self.root.search(query.strip(), max_distance)\n",
    "        # Sort by distance (closest matches first)\n",
    "        return sorted(results, key=lambda x: x[1])\n",
    "\n",
    "    def get_best_match(\n",
    "        self, query: str, max_distance: int = 2\n",
    "    ) -> Optional[Tuple[str, int]]:\n",
    "        \"\"\"Get the best (closest) match for query\"\"\"\n",
    "        results = self.search(query, max_distance)\n",
    "        return results[0] if results else None\n",
    "\n",
    "    def get_exact_match(self, query: str) -> Optional[str]:\n",
    "        \"\"\"Get exact match if exists\"\"\"\n",
    "        results = self.search(query, max_distance=0)\n",
    "        return results[0][0] if results else None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a400a48b",
   "metadata": {},
   "source": [
    "## Normalization Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e2622f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_input(address_input: str) -> str:\n",
    "    \"\"\"\n",
    "    Normalize Vietnamese address input for consistent parsing.\n",
    "\n",
    "    This function standardizes address strings by:\n",
    "    1. Cleaning whitespace, dot at last word and converting to lowercase\n",
    "    2. Adding proper spacing around punctuation\n",
    "    3. Expanding abbreviated prefixes with dots + adding commas\n",
    "    4. Handling prefix+digit combinations + adding commas\n",
    "    5. Separating concatenated prefecture words from location names\n",
    "    6. Handling abbreviated concatenated prefixes + adding commas\n",
    "    7. Final comma cleanup and validation\n",
    "\n",
    "    Args:\n",
    "        address_input (str): Raw Vietnamese address string\n",
    "\n",
    "    Returns:\n",
    "        str: Normalized address string ready for component extraction\n",
    "\n",
    "    Examples:\n",
    "        >>> normalize_input(\"P1,Q1,TPHCM\")\n",
    "        \"p. 1, q. 1, tp. hcm\"\n",
    "\n",
    "        >>> normalize_input(\"p tan binh q binh thanh\")\n",
    "        \"p. tan binh, q. binh thanh\"\n",
    "\n",
    "        >>> normalize_input(\"x my thành huyện cai lậy\")\n",
    "        \"x. my thành, huyện cai lậy\"\n",
    "    \"\"\"\n",
    "\n",
    "    def add_commas_before_admin_prefixes(text: str) -> str:\n",
    "        \"\"\"\n",
    "        Helper function to add commas before administrative prefixes with improved logic\n",
    "        \"\"\"\n",
    "        # More precise admin prefix pattern - only match actual administrative prefixes\n",
    "        admin_prefixes = r\"\\b(huyện|quận|phường|xã|thị xã|thành phố|tỉnh|thị trấn|tp\\.|tx\\.|tt\\.|p\\.|q\\.|h\\.|x\\.|t\\.)\\s+\"\n",
    "\n",
    "        matches = list(re.finditer(admin_prefixes, text, flags=re.IGNORECASE))\n",
    "\n",
    "        if matches:\n",
    "            # Process matches from right to left to avoid position shifts\n",
    "            for match in reversed(matches):\n",
    "                start_pos = match.start()\n",
    "\n",
    "                # Only add comma if the prefix is not at the beginning\n",
    "                if start_pos > 0:\n",
    "                    char_before = text[start_pos - 1]\n",
    "                    chars_before_2 = (\n",
    "                        text[max(0, start_pos - 2) : start_pos]\n",
    "                        if start_pos >= 2\n",
    "                        else \"\"\n",
    "                    )\n",
    "\n",
    "                    # Get the matched prefix for more precise logic\n",
    "                    matched_prefix_text = text[start_pos : match.end()].lower()\n",
    "\n",
    "                    # Skip inserting comma between 't.' and following 'p.'/'phố'/'x.' (for thành phố, thị xã)\n",
    "                    is_after_t = (\n",
    "                        re.search(r\"\\bt\\.\\s*$\", text[:start_pos], flags=re.IGNORECASE)\n",
    "                        is not None\n",
    "                    )\n",
    "                    is_follow_token = (\n",
    "                        re.match(\n",
    "                            r\"^(p\\.|phố|x\\.)\\s+\",\n",
    "                            matched_prefix_text,\n",
    "                            flags=re.IGNORECASE,\n",
    "                        )\n",
    "                        is not None\n",
    "                    )\n",
    "\n",
    "                    if is_after_t and is_follow_token:\n",
    "                        continue\n",
    "\n",
    "                    # Don't add comma if already preceded by comma or if preceded by a letter (part of same word)\n",
    "                    if (\n",
    "                        char_before != \",\"\n",
    "                        and chars_before_2 != \", \"\n",
    "                        and char_before == \" \"\n",
    "                    ):\n",
    "                        text = text[: start_pos - 1] + \", \" + text[start_pos:]\n",
    "\n",
    "        return text\n",
    "\n",
    "    # Step 1: Basic normalization - lowercase, clean whitespace, remove \"-\" and dot at last word\n",
    "    normalized = \" \".join(address_input.lower().split())\n",
    "    normalized = normalized.replace(\"-\", \"\")\n",
    "    normalized = normalized.rstrip(\".\")\n",
    "    # print(f\"Step 1 - Basic normalization: '{normalized}'\")\n",
    "\n",
    "    # Step 2a: Add space after commas for consistent parsing\n",
    "    normalized = re.sub(r\",(?=\\S)\", \", \", normalized)\n",
    "    # print(f\"Step 2a - After adding space after commas: '{normalized}'\")\n",
    "\n",
    "    # Step 2b: Add space after dots\n",
    "    normalized = re.sub(r\"\\.(?=\\S)\", \". \", normalized)\n",
    "    # print(f\"Step 2b - After adding space after dots: '{normalized}'\")\n",
    "\n",
    "    # Step 3: Expand standalone prefix letters with dots\n",
    "    # Only match letters followed by SPACE and then letters (not digits)\n",
    "    normalized = re.sub(r\"\\b([xhqtp])\\s+(?=[a-zA-ZÀ-ỹ])\", r\"\\1. \", normalized)\n",
    "    # print(f\"Step 3 - After expanding standalone prefixes: '{normalized}'\")\n",
    "\n",
    "    # Step 3b: Add commas immediately after creating new prefixes\n",
    "    normalized = add_commas_before_admin_prefixes(normalized)\n",
    "    # print(f\"Step 3b - After adding commas: '{normalized}'\")\n",
    "\n",
    "    # Step 4: Handle prefix letters next to digit\n",
    "    # First handle concatenated cases like \"q1p2\" -> \"q. 1 p. 2\"\n",
    "    normalized = re.sub(r\"([pq])(\\d{1,2})([pq]\\d{1,2})\", r\"\\1. \\2 \\3\", normalized)\n",
    "    # Then handle the general case with word boundaries\n",
    "    normalized = re.sub(r\"\\b([pq])(\\d{1,2})\\b\", r\"\\1. \\2\", normalized)\n",
    "    # Also handle cases where digits are followed by letters (like \"p1huyện\")\n",
    "    normalized = re.sub(r\"([pq])(\\d{1,2})([a-zA-ZÀ-ỹ])\", r\"\\1. \\2 \\3\", normalized)\n",
    "    # print(f\"Step 4 - After handling prefix+digit: '{normalized}'\")\n",
    "\n",
    "    # Step 4b: Add commas immediately after creating new prefixes\n",
    "    normalized = add_commas_before_admin_prefixes(normalized)\n",
    "    # print(f\"Step 4b - After adding commas: '{normalized}'\")\n",
    "\n",
    "    # Step 5: Separate concatenated full prefecture words from location names\n",
    "    normalized = re.sub(\n",
    "        r\"\\b(huyện|quận|phường|xã|thị xã|thành phố|tỉnh|thị trấn)([a-zA-ZÀ-ỹ])\",\n",
    "        r\"\\1 \\2\",\n",
    "        normalized,\n",
    "    )\n",
    "    # print(f\"Step 5 - After separating concatenated words: '{normalized}'\")\n",
    "\n",
    "    # Step 6: Handle abbreviated concatenated prefixes\n",
    "    # Match multi-letter abbreviations only\n",
    "    normalized = re.sub(r\"\\b(tp|tx|tt)([a-zA-ZÀ-ỹ])\", r\"\\1. \\2\", normalized)\n",
    "    # print(f\"Step 6 - After handling 'tp', 'tx', 'tt': '{normalized}'\")\n",
    "\n",
    "    # Step 6b: Add commas immediately after creating new prefixes\n",
    "    normalized = add_commas_before_admin_prefixes(normalized)\n",
    "    # print(f\"Step 6b - After adding commas: '{normalized}'\")\n",
    "\n",
    "    # Step 6c: Merge split abbreviations like 't. p.' to 'tp.'\n",
    "    normalized = re.sub(r\"t\\.\\s+p\\.\", \"tp.\", normalized)\n",
    "    normalized = re.sub(r\"t\\.\\s+x\\.\", \"tx.\", normalized)\n",
    "    normalized = re.sub(r\"t\\.\\s+t\\.\", \"tt.\", normalized)\n",
    "    # print(f\"Step 6c - After merging split abbreviations: '{normalized}'\")\n",
    "\n",
    "    # Step 7: Final cleanup and validation\n",
    "    # Remove any double spaces and trim\n",
    "    normalized = re.sub(r\"\\s+\", \" \", normalized).strip()\n",
    "\n",
    "    # Final check: ensure no double commas or misplaced commas\n",
    "    normalized = re.sub(r\",\\s*,\", \",\", normalized)  # Remove double commas\n",
    "    normalized = re.sub(r\",\\s*$\", \"\", normalized)  # Remove trailing comma\n",
    "    print(f\"NORMALIZED: '{normalized}'\")\n",
    "\n",
    "    return normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52c41224",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_abbreviations(normalized: str) -> str:\n",
    "    for alias, full in common_province_alias_map.items():\n",
    "        normalized = re.sub(\n",
    "            r\"\\b\" + re.escape(alias) + r\"\\b\", full, normalized, flags=re.IGNORECASE\n",
    "        )\n",
    "\n",
    "    admin_map = {\n",
    "        \"p.\": \"phường\",\n",
    "        \"q.\": \"quận\",\n",
    "        \"h.\": \"huyện\",\n",
    "        \"x.\": \"xã\",\n",
    "        \"tp.\": \"thành phố\",\n",
    "        \"t.ph\": \"thành phố\",\n",
    "        \"tx.\": \"thị xã\",\n",
    "        \"t.x.\": \"thị xã\",\n",
    "        \"tt.\": \"thị trấn\",\n",
    "        \"t.\": \"tỉnh\",\n",
    "    }\n",
    "\n",
    "    for abbrev, full in sorted(admin_map.items(), key=lambda x: -len(x[0])):\n",
    "        normalized = re.sub(\n",
    "            re.escape(abbrev), full + \" \", normalized, flags=re.IGNORECASE\n",
    "        )\n",
    "\n",
    "    normalized = re.sub(r\"\\s+\", \" \", normalized).strip()\n",
    "\n",
    "    return normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a37e455",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT: 'P1,Q1,TPHCM'\n",
      "NORMALIZED: 'p. 1, q. 1, tp. hcm'\n",
      "EXPANDED: 'phường 1, quận 1, thành phố hồ chí minh'\n",
      "-----\n",
      "INPUT: 'p tan binh q binh thanh'\n",
      "NORMALIZED: 'p. tan binh, q. binh thanh'\n",
      "EXPANDED: 'phường tan binh, quận binh thanh'\n",
      "-----\n",
      "INPUT: 'x my thành huyện cai lậy'\n",
      "NORMALIZED: 'x. my thành, huyện cai lậy'\n",
      "EXPANDED: 'xã my thành, huyện cai lậy'\n",
      "-----\n",
      "INPUT: 'hcm'\n",
      "NORMALIZED: 'hcm'\n",
      "EXPANDED: 'hồ chí minh'\n",
      "-----\n",
      "INPUT: 'hn'\n",
      "NORMALIZED: 'hn'\n",
      "EXPANDED: 'hà nội'\n",
      "-----\n",
      "INPUT: 'hnoi'\n",
      "NORMALIZED: 'hnoi'\n",
      "EXPANDED: 'hà nội'\n",
      "-----\n",
      "INPUT: 't.t.h'\n",
      "NORMALIZED: 't., t. h'\n",
      "EXPANDED: 'tỉnh , tỉnh h'\n",
      "-----\n",
      "INPUT: 'tp hcm'\n",
      "NORMALIZED: 'tp hcm'\n",
      "EXPANDED: 'tp hồ chí minh'\n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "tests = [\n",
    "    \"P1,Q1,TPHCM\",\n",
    "    \"p tan binh q binh thanh\",\n",
    "    \"x my thành huyện cai lậy\",\n",
    "    \"hcm\",\n",
    "    \"hn\",\n",
    "    \"hnoi\",\n",
    "    \"t.t.h\",\n",
    "    \"tp hcm\",\n",
    "]\n",
    "\n",
    "for test in tests:\n",
    "    print(f\"INPUT: '{test}'\")\n",
    "    normalized = normalize_input(test)\n",
    "    expanded = expand_abbreviations(normalized)\n",
    "    print(f\"EXPANDED: '{expanded}'\")\n",
    "    print(\"-----\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e09856",
   "metadata": {},
   "source": [
    "## Spelling Correction Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170df8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "\n",
    "\n",
    "def load_vietnamese_dictionary(\n",
    "    dict_path: str = \"data/vietnamese_address_dictionary.txt\",\n",
    ") -> List[str]:\n",
    "    \"\"\"Load Vietnamese dictionary from file\"\"\"\n",
    "    try:\n",
    "        with open(dict_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            words = [line.strip().lower() for line in f if line.strip()]\n",
    "        print(f\"Loaded {len(words)} words from Vietnamese dictionary\")\n",
    "        return words\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Dictionary file {dict_path} not found\")\n",
    "        return []\n",
    "\n",
    "\n",
    "def build_spelling_correction_trie() -> BKTree:\n",
    "    \"\"\"Build BK-Trie for spelling correction using Vietnamese dictionary\"\"\"\n",
    "    dictionary_words = load_vietnamese_dictionary()\n",
    "    dictionary_words = dictionary_words + list(common_province_alias_map.keys())\n",
    "    # Add common street keywords\n",
    "    dictionary_words = dictionary_words + street_keywords\n",
    "    dictionary_words = list(set(dictionary_words))\n",
    "    if not dictionary_words:\n",
    "        return BKTree()\n",
    "\n",
    "    spelling_trie = BKTree(dictionary_words)\n",
    "    print(f\"Built spelling correction trie with {spelling_trie.word_count} words\")\n",
    "    return spelling_trie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44a06bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "spelling_trie = build_spelling_correction_trie()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b6c5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_valid_vietnamese_word(word: str, spelling_trie: BKTree) -> bool:\n",
    "    \"\"\"Check if a word exists in the Vietnamese dictionary\"\"\"\n",
    "    if not word or not word.strip():\n",
    "        return True  # Consider empty words as valid (no correction needed)\n",
    "\n",
    "    # Check for exact match (case-insensitive)\n",
    "    exact_match = spelling_trie.get_exact_match(word.lower())\n",
    "    return exact_match is not None\n",
    "\n",
    "\n",
    "def calculate_word_similarity_score(original: str, candidate: str) -> float:\n",
    "    \"\"\"\n",
    "    Calculate similarity score between original and candidate word.\n",
    "    Higher score means better match.\n",
    "    \"\"\"\n",
    "    if not original or not candidate:\n",
    "        return 0.0\n",
    "\n",
    "    # Prefer longer matches\n",
    "    length_bonus = min(len(candidate), len(original)) / max(\n",
    "        len(candidate), len(original)\n",
    "    )\n",
    "\n",
    "    # Prefer words that start with the same characters\n",
    "    common_prefix = 0\n",
    "    for i, (c1, c2) in enumerate(zip(original, candidate)):\n",
    "        if c1 == c2:\n",
    "            common_prefix += 1\n",
    "        else:\n",
    "            break\n",
    "    prefix_score = common_prefix / max(len(original), len(candidate))\n",
    "\n",
    "    # Calculate overall similarity\n",
    "    edit_distance = levenshtein_distance(original, candidate)\n",
    "    max_length = max(len(original), len(candidate))\n",
    "    distance_score = 1 - (edit_distance / max_length) if max_length > 0 else 0\n",
    "\n",
    "    # Combine scores (weighted)\n",
    "    final_score = (distance_score * 0.6) + (prefix_score * 0.3) + (length_bonus * 0.1)\n",
    "    return final_score\n",
    "\n",
    "\n",
    "def correct_word(\n",
    "    word: str, spelling_trie: BKTree, max_distance: int = 2\n",
    ") -> Tuple[str, bool, int]:\n",
    "    \"\"\"\n",
    "    Correct a single word using the spelling trie with improved scoring\n",
    "\n",
    "    Args:\n",
    "        word: Word to correct\n",
    "        spelling_trie: BK-Trie containing dictionary words\n",
    "        max_distance: Maximum edit distance for suggestions\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (corrected_word, was_corrected, edit_distance)\n",
    "    \"\"\"\n",
    "    if not word or not word.strip():\n",
    "        return word, False, 0\n",
    "\n",
    "    word_lower = word.lower().strip()\n",
    "\n",
    "    # Skip numbers and very short words\n",
    "    if word_lower.isdigit() or len(word_lower) < 2:\n",
    "        return word, False, 0\n",
    "\n",
    "    # Skip common punctuation and special characters\n",
    "    if word_lower in {\"-\", \".\", \",\", \"/\", \"\\\\\", \"(\", \")\", \"[\", \"]\", \"{\", \"}\"}:\n",
    "        return word, False, 0\n",
    "\n",
    "    # Check if word is already correct\n",
    "    if is_valid_vietnamese_word(word_lower, spelling_trie):\n",
    "        return word, False, 0\n",
    "\n",
    "    # Find spelling corrections\n",
    "    matches = spelling_trie.search(word_lower, max_distance)\n",
    "\n",
    "    if matches:\n",
    "        # Score all matches and pick the best one\n",
    "        scored_matches = []\n",
    "        for candidate, distance in matches:\n",
    "            similarity_score = calculate_word_similarity_score(word_lower, candidate)\n",
    "            # Combine distance and similarity (lower distance is better, higher similarity is better)\n",
    "            combined_score = similarity_score - (\n",
    "                distance * 0.1\n",
    "            )  # Penalize distance slightly\n",
    "            scored_matches.append(\n",
    "                (candidate, distance, similarity_score, combined_score)\n",
    "            )\n",
    "\n",
    "        # Sort by combined score (descending)\n",
    "        scored_matches.sort(key=lambda x: x[3], reverse=True)\n",
    "\n",
    "        best_match, distance, similarity, combined = scored_matches[0]\n",
    "\n",
    "        # Only suggest correction if it's reasonable\n",
    "        if distance <= max_distance and similarity > 0.3:\n",
    "            return best_match, True, distance\n",
    "\n",
    "    # No good correction found\n",
    "    return word, False, float(\"inf\")\n",
    "\n",
    "\n",
    "def correct_address_spelling(\n",
    "    address: str, spelling_trie: BKTree, max_distance: int = 2, debug: bool = False\n",
    ") -> Tuple[str, List[dict]]:\n",
    "    \"\"\"\n",
    "    Correct spelling errors in Vietnamese address text\n",
    "\n",
    "    Args:\n",
    "        address: Input address string\n",
    "        spelling_trie: BK-Trie for spell checking\n",
    "        max_distance: Maximum edit distance for corrections\n",
    "        debug: Whether to print debug information\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (corrected_address, corrections_made)\n",
    "    \"\"\"\n",
    "    if not address or not address.strip():\n",
    "        return address, []\n",
    "\n",
    "    # Tokenize the address while preserving punctuation and spacing\n",
    "    # Split on whitespace but keep track of original spacing\n",
    "    tokens = re.findall(r\"\\S+|\\s+\", address)\n",
    "\n",
    "    corrected_tokens = []\n",
    "    corrections_made = []\n",
    "\n",
    "    for i, token in enumerate(tokens):\n",
    "        if token.isspace():\n",
    "            # Preserve whitespace as-is\n",
    "            corrected_tokens.append(token)\n",
    "            continue\n",
    "\n",
    "        # Clean token of punctuation for spell checking\n",
    "        clean_token = re.sub(\n",
    "            r\"[^\\w\\sàáạảãâầấậẩẫăằắặẳẵèéẹẻẽêềếệểễìíịỉĩòóọỏõôồốộổỗơờớợởỡùúụủũưừứựửữỳýỵỷỹđÀÁẠẢÃÂẦẤẬẨẪĂẰẮẶẲẴÈÉẸẺẼÊỀẾỆỂỄÌÍỊỈĨÒÓỌỎÕÔỒỐỘỔỖƠỜỚỢỞỠÙÚỤỦŨƯỪỨỰỬỮỲÝỴỶỸĐ]\",\n",
    "            \"\",\n",
    "            token,\n",
    "        )\n",
    "\n",
    "        if clean_token:\n",
    "            corrected_word, was_corrected, distance = correct_word(\n",
    "                clean_token, spelling_trie, max_distance\n",
    "            )\n",
    "\n",
    "            if was_corrected:\n",
    "                # Replace the clean part while preserving punctuation\n",
    "                corrected_token = token.replace(clean_token, corrected_word)\n",
    "                corrected_tokens.append(corrected_token)\n",
    "\n",
    "                correction_info = {\n",
    "                    \"position\": i,\n",
    "                    \"original\": clean_token,\n",
    "                    \"corrected\": corrected_word,\n",
    "                    \"distance\": distance,\n",
    "                    \"full_token_original\": token,\n",
    "                    \"full_token_corrected\": corrected_token,\n",
    "                }\n",
    "                corrections_made.append(correction_info)\n",
    "\n",
    "                if debug:\n",
    "                    print(\n",
    "                        f\"Corrected: '{clean_token}' -> '{corrected_word}' (distance: {distance})\"\n",
    "                    )\n",
    "            else:\n",
    "                corrected_tokens.append(token)\n",
    "        else:\n",
    "            # Token is only punctuation\n",
    "            corrected_tokens.append(token)\n",
    "\n",
    "    corrected_address = \"\".join(corrected_tokens)\n",
    "\n",
    "    if debug and corrections_made:\n",
    "        print(f\"Original: {address}\")\n",
    "        print(f\"Corrected: {corrected_address}\")\n",
    "        print(f\"Made {len(corrections_made)} corrections\")\n",
    "\n",
    "    return corrected_address, corrections_made\n",
    "\n",
    "\n",
    "# Build the spelling correction trie if not already built\n",
    "if \"spelling_trie\" not in locals():\n",
    "    print(\"Building spelling correction trie...\")\n",
    "    spelling_trie = build_spelling_correction_trie()\n",
    "    print(\"Spelling correction trie ready!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c58bad",
   "metadata": {},
   "source": [
    "## Suggestion Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63819e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_component(text, component_type):\n",
    "    \"\"\"Remove prefix patterns from component text\"\"\"\n",
    "    if not text:\n",
    "        return None\n",
    "\n",
    "    text = text.strip()\n",
    "    for pattern in administration_prefix_patterns[component_type]:\n",
    "        text = re.sub(pattern, \"\", text, flags=re.IGNORECASE).strip()\n",
    "\n",
    "    return text if text else None\n",
    "\n",
    "\n",
    "def check_single_letter_prefix(word):\n",
    "    \"\"\"\n",
    "    Check if a word starts with a single letter prefix that could be an abbreviation.\n",
    "    Returns tuple: (has_prefix, prefix_letter, remaining_word)\n",
    "\n",
    "    Designed to catch cases like:\n",
    "    - \"tđắk\" -> (\"t\", \"đắk\") for \"tỉnh đắk\"\n",
    "    - \"hkrông\" -> (\"h\", \"krông\") for \"huyện krông\"\n",
    "    - \"qbình\" -> (\"q\", \"bình\") for \"quận bình\"\n",
    "\n",
    "    But NOT cases like:\n",
    "    - \"thị\" (part of \"thị trấn\")\n",
    "    - \"thành\" (part of \"thành phố\")\n",
    "    - Standard abbreviations like \"tp.\", \"q.\", \"h.\", \"p.\", \"x.\", \"tx.\", \"tt.\" (handled elsewhere)\n",
    "    \"\"\"\n",
    "    # Ignore known dotted abbreviations entirely (handled by explicit rules)\n",
    "    if re.match(r\"^(tp|tx|tt|q|h|p|x)\\.$\", word, flags=re.IGNORECASE):\n",
    "        return False, None, word\n",
    "\n",
    "    if len(word) > 2 and word[0] in SINGLE_LETTER_PREFIXES:\n",
    "        # Exclude full administrative words\n",
    "        full_admin_prefixes = [\"thành\", \"thị\", \"huyện\", \"quận\", \"phường\", \"tỉnh\"]\n",
    "        for prefix in full_admin_prefixes:\n",
    "            if word.lower().startswith(prefix):\n",
    "                return False, None, word\n",
    "\n",
    "        # Require the remainder to look like a place token: has a space or VN diacritics\n",
    "        remaining = word[1:]\n",
    "        if not re.search(r\"[à-ỹ\\s]\", remaining, flags=re.IGNORECASE):\n",
    "            return False, None, word\n",
    "\n",
    "        # Also avoid too-short remainders\n",
    "        if len(remaining) < 2:\n",
    "            return False, None, word\n",
    "\n",
    "        return True, word[0], remaining\n",
    "    return False, None, word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b303a35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def suggest_address_components(address_input):\n",
    "    \"\"\"\n",
    "    Heuristically suggest province, district, and ward from a (normalized) Vietnamese address.\n",
    "    Processes right-to-left (common in VN addresses).\n",
    "    Now includes fallback logic for parts without explicit prefixes and alias mapping.\n",
    "    \"\"\"\n",
    "\n",
    "    # --- Small helpers -----------------------------------------------------\n",
    "    def add_component(bucket, key, value):\n",
    "        if value is None:\n",
    "            return\n",
    "        value = value.strip()\n",
    "        if not value:\n",
    "            return\n",
    "        if value not in bucket[key]:\n",
    "            bucket[key].append(value)\n",
    "\n",
    "    def take_remaining_words(words, start_idx, used_indices):\n",
    "        return \" \".join(\n",
    "            [\n",
    "                w\n",
    "                for j, w in enumerate(words[start_idx:], start=start_idx)\n",
    "                if j not in used_indices\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def mark_used_range(used_indices, start_idx, end_idx):\n",
    "        for j in range(start_idx, end_idx):\n",
    "            used_indices.add(j)\n",
    "\n",
    "    def classify_and_add_by_letter(prefix_letter, text):\n",
    "        nonlocal province_assigned\n",
    "        if prefix_letter == \"t\":  # tỉnh\n",
    "            add_component(result, \"province\", clean_component(text, \"province\"))\n",
    "            province_assigned = True\n",
    "        elif prefix_letter in [\"q\", \"h\"]:  # quận, huyện\n",
    "            add_component(result, \"district\", clean_component(text, \"district\"))\n",
    "        elif prefix_letter in [\"p\", \"x\"]:  # phường, xã\n",
    "            add_component(result, \"ward\", clean_component(text, \"ward\"))\n",
    "\n",
    "    def word_matches_any(patterns, word):\n",
    "        return any(re.match(pat, word, flags=re.IGNORECASE) for pat in patterns)\n",
    "\n",
    "    def contains_unrelated_keywords(text):\n",
    "        \"\"\"Check if text contains street/address keywords that should not be administrative components\"\"\"\n",
    "        text_lower = text.lower()\n",
    "        return any(keyword in text_lower for keyword in street_keywords)\n",
    "\n",
    "    def apply_alias_mapping(result):\n",
    "        \"\"\"Apply known province aliases to improve province identification\"\"\"\n",
    "        # Check each province and try to map aliases\n",
    "        for i, province in enumerate(result[\"province\"]):\n",
    "            province_lower = province.lower().strip()\n",
    "\n",
    "            # Direct lookup in alias map\n",
    "            if province_lower in common_province_alias_map:\n",
    "                result[\"province\"][i] = common_province_alias_map[province_lower]\n",
    "                continue\n",
    "\n",
    "            # Check for partial matches or common variations\n",
    "            for alias, full_name in common_province_alias_map.items():\n",
    "                if alias in province_lower or province_lower in alias:\n",
    "                    result[\"province\"][i] = full_name\n",
    "                    break\n",
    "\n",
    "        return result\n",
    "\n",
    "    # --- Output bucket -----------------------------------------------------\n",
    "    result = {\n",
    "        \"province\": [],\n",
    "        \"district\": [],\n",
    "        \"ward\": [],\n",
    "        \"normalized_raw_input\": address_input,\n",
    "        \"remaining\": None,\n",
    "        \"remaining_parts\": [],\n",
    "    }\n",
    "    province_assigned = False\n",
    "\n",
    "    # --- No-comma path: token-by-token right-to-left ----------------------\n",
    "    if \",\" not in address_input:\n",
    "        words = address_input.split()\n",
    "        used_indices = set()\n",
    "\n",
    "        for i in reversed(range(len(words))):\n",
    "            if i in used_indices:\n",
    "                continue\n",
    "\n",
    "            word = words[i]\n",
    "\n",
    "            # Handle multi-token admin prefixes: \"t. p.\" / \"t. phố\" or \"t. x.\"\n",
    "            if (\n",
    "                i + 1 < len(words)\n",
    "                and words[i] == \"t.\"\n",
    "                and words[i + 1] in {\"p.\", \"phố\", \"x.\"}\n",
    "            ):\n",
    "                remaining_text = (\n",
    "                    take_remaining_words(words, i + 2, used_indices)\n",
    "                    if i + 2 < len(words)\n",
    "                    else \"\"\n",
    "                )\n",
    "                if words[i + 1] in {\"p.\", \"phố\"}:  # thành phố\n",
    "                    if not province_assigned:\n",
    "                        add_component(\n",
    "                            result,\n",
    "                            \"province\",\n",
    "                            clean_component(remaining_text, \"province\"),\n",
    "                        )\n",
    "                        province_assigned = True\n",
    "                    else:\n",
    "                        add_component(\n",
    "                            result,\n",
    "                            \"district\",\n",
    "                            clean_component(remaining_text, \"district\"),\n",
    "                        )\n",
    "                else:  # thị xã → district-level\n",
    "                    add_component(\n",
    "                        result, \"district\", clean_component(remaining_text, \"district\")\n",
    "                    )\n",
    "                mark_used_range(used_indices, i, len(words))\n",
    "                continue\n",
    "\n",
    "            # Single-letter prefix at word start (e.g., \"tđắk\")\n",
    "            has_prefix, prefix_letter, remaining_word = check_single_letter_prefix(word)\n",
    "            if has_prefix:\n",
    "                component_text = \" \".join(\n",
    "                    [remaining_word]\n",
    "                    + [\n",
    "                        words[j]\n",
    "                        for j in range(i + 1, len(words))\n",
    "                        if j not in used_indices\n",
    "                    ]\n",
    "                )\n",
    "                classify_and_add_by_letter(prefix_letter, component_text)\n",
    "                mark_used_range(used_indices, i, len(words))\n",
    "                continue\n",
    "\n",
    "            # Standalone single-letter prefix token (e.g., \"t\")\n",
    "            if len(word) == 1 and word in SINGLE_LETTER_PREFIXES and i < len(words) - 1:\n",
    "                remaining_text = take_remaining_words(words, i, used_indices)\n",
    "                if word == \"t\":\n",
    "                    add_component(\n",
    "                        result, \"province\", clean_component(remaining_text, \"province\")\n",
    "                    )\n",
    "                    province_assigned = True\n",
    "                elif word in [\"q\", \"h\"]:\n",
    "                    add_component(\n",
    "                        result, \"district\", clean_component(remaining_text, \"district\")\n",
    "                    )\n",
    "                elif word in [\"p\", \"x\"]:\n",
    "                    add_component(\n",
    "                        result, \"ward\", clean_component(remaining_text, \"ward\")\n",
    "                    )\n",
    "                mark_used_range(used_indices, i, len(words))\n",
    "                continue\n",
    "\n",
    "            # Regular explicit prefix tokens (exact-token match lists)\n",
    "            province_tokens = [r\"^(thành phố|tỉnh|tp|tp\\.|t\\.)$\"]\n",
    "            district_tokens = [r\"^(huyện|quận|thị xã|thành phố|q\\.|h\\.|tx\\.|tp\\.)$\"]\n",
    "            ward_tokens = [r\"^(phường|xã|thị trấn|p\\.|x\\.|tt\\.)$\"]\n",
    "\n",
    "            if word_matches_any(province_tokens, word):\n",
    "                remaining_text = take_remaining_words(words, i, used_indices)\n",
    "                add_component(\n",
    "                    result, \"province\", clean_component(remaining_text, \"province\")\n",
    "                )\n",
    "                province_assigned = True\n",
    "                mark_used_range(used_indices, i, len(words))\n",
    "                continue\n",
    "\n",
    "            if i not in used_indices and word_matches_any(district_tokens, word):\n",
    "                remaining_text = take_remaining_words(words, i, used_indices)\n",
    "                add_component(\n",
    "                    result, \"district\", clean_component(remaining_text, \"district\")\n",
    "                )\n",
    "                mark_used_range(used_indices, i, len(words))\n",
    "                continue\n",
    "\n",
    "            if i not in used_indices and word_matches_any(ward_tokens, word):\n",
    "                remaining_text = take_remaining_words(words, i, used_indices)\n",
    "                add_component(result, \"ward\", clean_component(remaining_text, \"ward\"))\n",
    "                mark_used_range(used_indices, i, len(words))\n",
    "                continue\n",
    "\n",
    "        remaining_words = [\n",
    "            words[idx] for idx in range(len(words)) if idx not in used_indices\n",
    "        ]\n",
    "        remaining_str = \" \".join(remaining_words) if remaining_words else None\n",
    "        result[\"remaining\"] = remaining_str\n",
    "        result[\"remaining_parts\"] = [remaining_str] if remaining_str else []\n",
    "\n",
    "        # Apply alias mapping before returning\n",
    "        result = apply_alias_mapping(result)\n",
    "        return result\n",
    "\n",
    "    # --- Comma-separated path: part-by-part right-to-left ------------------\n",
    "    parts = [part.strip() for part in address_input.split(\",\")]\n",
    "    parts = [part for part in parts if part]\n",
    "\n",
    "    # Keep track of matched parts and unmatched parts\n",
    "    matched_parts = []\n",
    "    unmatched_parts = []\n",
    "\n",
    "    for i, part in enumerate(reversed(parts)):\n",
    "        matched = False\n",
    "        words_in_part = part.split()\n",
    "\n",
    "        # Intentionally disable single-letter heuristic in comma-separated parts\n",
    "        # Rely only on explicit multi-token and prefix-pattern checks below\n",
    "\n",
    "        if not matched:\n",
    "            # Handle leading multi-token admin prefixes inside a part\n",
    "            if re.match(r\"^\\s*t\\.\\s*(p\\.|phố)\\s+\", part, flags=re.IGNORECASE):\n",
    "                component_text = re.sub(\n",
    "                    r\"^\\s*t\\.\\s*(p\\.|phố)\\s+\", \"\", part, flags=re.IGNORECASE\n",
    "                )\n",
    "                if not province_assigned:\n",
    "                    add_component(\n",
    "                        result, \"province\", clean_component(component_text, \"province\")\n",
    "                    )\n",
    "                    province_assigned = True\n",
    "                else:\n",
    "                    add_component(\n",
    "                        result, \"district\", clean_component(component_text, \"district\")\n",
    "                    )\n",
    "                matched = True\n",
    "            elif re.match(r\"^\\s*t\\.\\s*x\\.\\s+\", part, flags=re.IGNORECASE):\n",
    "                component_text = re.sub(\n",
    "                    r\"^\\s*t\\.\\s*x\\.\\s+\", \"\", part, flags=re.IGNORECASE\n",
    "                )\n",
    "                add_component(\n",
    "                    result, \"district\", clean_component(component_text, \"district\")\n",
    "                )\n",
    "                matched = True\n",
    "\n",
    "            # City-level indicator at start: \"thành phố\" / \"tp.\" → district if province already assigned\n",
    "            elif re.match(\n",
    "                r\"^\\s*(thành phố|t\\. phố|t\\. p\\.|tp\\.?)[\\s]+\", part, flags=re.IGNORECASE\n",
    "            ):\n",
    "                if not province_assigned:\n",
    "                    add_component(result, \"province\", clean_component(part, \"province\"))\n",
    "                    province_assigned = True\n",
    "                else:\n",
    "                    add_component(result, \"district\", clean_component(part, \"district\"))\n",
    "                matched = True\n",
    "            # Match explicit admin prefixes by priority\n",
    "            elif any(\n",
    "                re.match(p, part, flags=re.IGNORECASE)\n",
    "                for p in administration_prefix_patterns[\"province\"]\n",
    "            ):\n",
    "                add_component(result, \"province\", clean_component(part, \"province\"))\n",
    "                province_assigned = True\n",
    "                matched = True\n",
    "            elif any(\n",
    "                re.match(p, part, flags=re.IGNORECASE)\n",
    "                for p in administration_prefix_patterns[\"district\"]\n",
    "            ):\n",
    "                add_component(result, \"district\", clean_component(part, \"district\"))\n",
    "                matched = True\n",
    "            elif any(\n",
    "                re.match(p, part, flags=re.IGNORECASE)\n",
    "                for p in administration_prefix_patterns[\"ward\"]\n",
    "            ):\n",
    "                add_component(result, \"ward\", clean_component(part, \"ward\"))\n",
    "                matched = True\n",
    "\n",
    "        if matched:\n",
    "            matched_parts.append(part)\n",
    "        else:\n",
    "            unmatched_parts.append(part)\n",
    "\n",
    "    # Fallback logic for unmatched parts (with street keyword filtering)\n",
    "    # Process unmatched parts from right to left (province -> district -> ward)\n",
    "    if unmatched_parts:\n",
    "        # Since we processed parts in reverse order, unmatched_parts are also in reverse order\n",
    "        # So the first unmatched part is the rightmost (likely province)\n",
    "        for i, part in enumerate(unmatched_parts):\n",
    "            # Skip parts that contain street/address keywords\n",
    "            if contains_unrelated_keywords(part):\n",
    "                result[\"remaining_parts\"].append(part)\n",
    "                continue\n",
    "\n",
    "            if i == 0 and not result[\"province\"]:\n",
    "                # First unmatched part (rightmost) -> province\n",
    "                add_component(result, \"province\", part)\n",
    "            elif i == 1 and not result[\"district\"]:\n",
    "                # Second unmatched part -> district\n",
    "                add_component(result, \"district\", part)\n",
    "            elif i == 2 and not result[\"ward\"]:\n",
    "                # Third unmatched part -> ward\n",
    "                add_component(result, \"ward\", part)\n",
    "            else:\n",
    "                # Any remaining parts go to remaining_parts\n",
    "                result[\"remaining_parts\"].append(part)\n",
    "\n",
    "    # Set remaining string for any leftover parts\n",
    "    if not result[\"remaining_parts\"]:\n",
    "        # If no fallback parts were used, use the old logic for remaining parts computation\n",
    "        used_parts = []\n",
    "        for part in parts:\n",
    "            part_used = False\n",
    "            words_in_part = part.split()\n",
    "            if words_in_part:\n",
    "                first_word = words_in_part[0]\n",
    "                has_prefix, _, _ = check_single_letter_prefix(first_word)\n",
    "                if has_prefix or (\n",
    "                    len(first_word) == 1 and first_word in SINGLE_LETTER_PREFIXES\n",
    "                ):\n",
    "                    used_parts.append(part)\n",
    "                    part_used = True\n",
    "            if not part_used:\n",
    "                if re.match(r\"^\\s*t\\.\\s*p\\.\\s+\", part, flags=re.IGNORECASE) or re.match(\n",
    "                    r\"^\\s*t\\.\\s*x\\.\\s+\", part, flags=re.IGNORECASE\n",
    "                ):\n",
    "                    used_parts.append(part)\n",
    "                    part_used = True\n",
    "            if not part_used:\n",
    "                for component_type in [\"district\", \"province\", \"ward\"]:\n",
    "                    if any(\n",
    "                        re.match(p, part, flags=re.IGNORECASE)\n",
    "                        for p in administration_prefix_patterns[component_type]\n",
    "                    ):\n",
    "                        used_parts.append(part)\n",
    "                        part_used = True\n",
    "                        break\n",
    "\n",
    "        # Add unmatched parts that were assigned by fallback logic to used_parts\n",
    "        used_parts.extend(matched_parts)\n",
    "        # Only add unmatched parts that don't contain street keywords and were assigned\n",
    "        for i, part in enumerate(unmatched_parts):\n",
    "            if not contains_unrelated_keywords(part) and i < 3:\n",
    "                used_parts.append(part)\n",
    "\n",
    "        remaining_parts = [p for p in parts if p not in used_parts]\n",
    "        result[\"remaining_parts\"] = remaining_parts\n",
    "\n",
    "    result[\"remaining\"] = (\n",
    "        \", \".join(result[\"remaining_parts\"]) if result[\"remaining_parts\"] else None\n",
    "    )\n",
    "\n",
    "    # Apply alias mapping before returning\n",
    "    result = apply_alias_mapping(result)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "# Example result\n",
    "# result = {'province': ['hồ chí minh'], 'district': ['1'], 'ward': [], 'normalized_raw_input': '161/18a cô giang, cô giang , quận 1, tp. hồ chí minh', 'remaining': 'cô giang, 161/18a cô giang', 'remaining_parts': ['cô giang', '161/18a cô giang']}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9bb54a",
   "metadata": {},
   "source": [
    "## Classification Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f3c71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from typing import List, Dict\n",
    "\n",
    "\n",
    "# def load_txt_with_encoding(file_path: str) -> List[str]:\n",
    "#     \"\"\"Load data from TXT file with automatic encoding detection\"\"\"\n",
    "#     encodings = [\"utf-8-sig\", \"utf-8\", \"latin-1\", \"cp1252\", \"iso-8859-1\"]\n",
    "\n",
    "#     for encoding in encodings:\n",
    "#         try:\n",
    "#             with open(file_path, \"r\", encoding=encoding) as f:\n",
    "#                 data = [line.strip() for line in f if line.strip()]\n",
    "#             return data\n",
    "#         except (UnicodeDecodeError, FileNotFoundError):\n",
    "#             continue\n",
    "\n",
    "#     raise Exception(f\"Could not read file {file_path} with any supported encoding\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f3c71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def build_classification_tries() -> Tuple[BKTree, BKTree, BKTree]:\n",
    "#     \"\"\"Build BK-Tries for provinces, districts, and wards\"\"\"\n",
    "#     print(\"Loading data files...\")\n",
    "\n",
    "#     # Load provinces\n",
    "#     provinces = load_txt_with_encoding(\"./data/provinces.txt\")\n",
    "#     print(f\"Loaded {len(provinces)} provinces\")\n",
    "\n",
    "#     # Load districts\n",
    "#     districts = load_txt_with_encoding(\"./data/districts.txt\")\n",
    "#     print(f\"Loaded {len(districts)} districts\")\n",
    "\n",
    "#     # Load wards\n",
    "#     wards = load_txt_with_encoding(\"./data/wards.txt\")\n",
    "#     print(f\"Loaded {len(wards)} wards\")\n",
    "\n",
    "#     print(\"\\nBuilding BK-Tries...\")\n",
    "\n",
    "#     # Build tries\n",
    "#     province_trie = BKTree(provinces)\n",
    "#     print(f\"Province trie built with {province_trie.word_count} entries\")\n",
    "\n",
    "#     district_trie = BKTree(districts)\n",
    "#     print(f\"District trie built with {district_trie.word_count} entries\")\n",
    "\n",
    "#     ward_trie = BKTree(wards)\n",
    "#     print(f\"Ward trie built with {ward_trie.word_count} entries\")\n",
    "\n",
    "#     return province_trie, district_trie, ward_trie\n",
    "\n",
    "\n",
    "# def classify_address_components(\n",
    "#     suggestion_result: Dict,\n",
    "#     province_trie: BKTree,\n",
    "#     district_trie: BKTree,\n",
    "#     ward_trie: BKTree,\n",
    "#     max_distance: int = 2,\n",
    "# ) -> Dict:\n",
    "#     \"\"\"\n",
    "#     Classify suggested address components using BK-Tries\n",
    "\n",
    "#     Args:\n",
    "#         suggestion_result: Result from suggest_address_components()\n",
    "#         province_trie, district_trie, ward_trie: BK-Tries for classification\n",
    "#         max_distance: Maximum edit distance for fuzzy matching\n",
    "\n",
    "#     Returns:\n",
    "#         Dict with classified components and confidence scores\n",
    "#     \"\"\"\n",
    "#     result = {\n",
    "#         \"province\": {\n",
    "#             \"original\": suggestion_result.get(\"province\", []),\n",
    "#             \"classified\": [],\n",
    "#             \"scores\": [],\n",
    "#         },\n",
    "#         \"district\": {\n",
    "#             \"original\": suggestion_result.get(\"district\", []),\n",
    "#             \"classified\": [],\n",
    "#             \"scores\": [],\n",
    "#         },\n",
    "#         \"ward\": {\n",
    "#             \"original\": suggestion_result.get(\"ward\", []),\n",
    "#             \"classified\": [],\n",
    "#             \"scores\": [],\n",
    "#         },\n",
    "#         \"remaining\": suggestion_result.get(\"remaining\"),\n",
    "#         \"remaining_parts\": suggestion_result.get(\"remaining_parts\", []),\n",
    "#     }\n",
    "\n",
    "#     # Classify provinces\n",
    "#     for province_suggestion in suggestion_result.get(\"province\", []):\n",
    "#         matches = province_trie.search(province_suggestion, max_distance)\n",
    "#         if matches:\n",
    "#             best_match, distance = matches[0]\n",
    "#             confidence = max(0, 1 - (distance / len(province_suggestion)))\n",
    "#             result[\"province\"][\"classified\"].append(best_match)\n",
    "#             result[\"province\"][\"scores\"].append(\n",
    "#                 {\n",
    "#                     \"match\": best_match,\n",
    "#                     \"distance\": distance,\n",
    "#                     \"confidence\": confidence,\n",
    "#                     \"all_matches\": matches[:3],  # Top 3 matches\n",
    "#                 }\n",
    "#             )\n",
    "#         else:\n",
    "#             result[\"province\"][\"classified\"].append(None)\n",
    "#             result[\"province\"][\"scores\"].append(\n",
    "#                 {\n",
    "#                     \"match\": None,\n",
    "#                     \"distance\": float(\"inf\"),\n",
    "#                     \"confidence\": 0,\n",
    "#                     \"all_matches\": [],\n",
    "#                 }\n",
    "#             )\n",
    "\n",
    "#     # Classify districts\n",
    "#     for district_suggestion in suggestion_result.get(\"district\", []):\n",
    "#         matches = district_trie.search(district_suggestion, max_distance)\n",
    "#         if matches:\n",
    "#             best_match, distance = matches[0]\n",
    "#             confidence = max(0, 1 - (distance / len(district_suggestion)))\n",
    "#             result[\"district\"][\"classified\"].append(best_match)\n",
    "#             result[\"district\"][\"scores\"].append(\n",
    "#                 {\n",
    "#                     \"match\": best_match,\n",
    "#                     \"distance\": distance,\n",
    "#                     \"confidence\": confidence,\n",
    "#                     \"all_matches\": matches[:3],\n",
    "#                 }\n",
    "#             )\n",
    "#         else:\n",
    "#             result[\"district\"][\"classified\"].append(None)\n",
    "#             result[\"district\"][\"scores\"].append(\n",
    "#                 {\n",
    "#                     \"match\": None,\n",
    "#                     \"distance\": float(\"inf\"),\n",
    "#                     \"confidence\": 0,\n",
    "#                     \"all_matches\": [],\n",
    "#                 }\n",
    "#             )\n",
    "\n",
    "#     # Classify wards\n",
    "#     for ward_suggestion in suggestion_result.get(\"ward\", []):\n",
    "#         matches = ward_trie.search(ward_suggestion, max_distance)\n",
    "#         if matches:\n",
    "#             best_match, distance = matches[0]\n",
    "#             confidence = max(0, 1 - (distance / len(ward_suggestion)))\n",
    "#             result[\"ward\"][\"classified\"].append(best_match)\n",
    "#             result[\"ward\"][\"scores\"].append(\n",
    "#                 {\n",
    "#                     \"match\": best_match,\n",
    "#                     \"distance\": distance,\n",
    "#                     \"confidence\": confidence,\n",
    "#                     \"all_matches\": matches[:3],\n",
    "#                 }\n",
    "#             )\n",
    "#         else:\n",
    "#             result[\"ward\"][\"classified\"].append(None)\n",
    "#             result[\"ward\"][\"scores\"].append(\n",
    "#                 {\n",
    "#                     \"match\": None,\n",
    "#                     \"distance\": float(\"inf\"),\n",
    "#                     \"confidence\": 0,\n",
    "#                     \"all_matches\": [],\n",
    "#                 }\n",
    "#             )\n",
    "\n",
    "#     return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1395a145",
   "metadata": {},
   "outputs": [],
   "source": [
    "# province_trie, district_trie, ward_trie = build_classification_tries()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce13462d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# suggestion = suggest_address_components(\n",
    "#     normalize_input(\"QL.9, Xã Phogn Mỹ, Huyện Phong Điền, Tỉnh Thừa Thiên Huế\")\n",
    "# )\n",
    "# result = classify_address_components(\n",
    "#     suggestion, province_trie, district_trie, ward_trie\n",
    "# )\n",
    "# print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f73718",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f10a7cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "\n",
    "\n",
    "# def load_test_cases(filename: str) -> List[Dict]:\n",
    "#     \"\"\"Load test cases from JSON file\"\"\"\n",
    "#     try:\n",
    "#         with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
    "#             return json.load(f)\n",
    "#     except FileNotFoundError:\n",
    "#         print(f\"Warning: {filename} not found, using empty list\")\n",
    "#         return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14dfb08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time\n",
    "# import csv\n",
    "# import os\n",
    "\n",
    "\n",
    "# def evaluate_prediction(predicted: Dict, expected: Dict) -> Dict:\n",
    "#     \"\"\"Evaluate a single prediction against expected results\"\"\"\n",
    "#     result = {\n",
    "#         \"province_correct\": False,\n",
    "#         \"district_correct\": False,\n",
    "#         \"ward_correct\": False,\n",
    "#         \"province_match\": None,\n",
    "#         \"district_match\": None,\n",
    "#         \"ward_match\": None,\n",
    "#     }\n",
    "\n",
    "#     # Check province\n",
    "#     if expected.get(\"province\"):\n",
    "#         predicted_provinces = predicted.get(\"province\", {}).get(\"classified\", [])\n",
    "#         if predicted_provinces and predicted_provinces[0]:\n",
    "#             predicted_province = predicted_provinces[0].lower().strip()\n",
    "#             expected_province = expected[\"province\"].lower().strip()\n",
    "#             if (\n",
    "#                 predicted_province == expected_province\n",
    "#                 or expected_province in predicted_province\n",
    "#                 or predicted_province in expected_province\n",
    "#             ):\n",
    "#                 result[\"province_correct\"] = True\n",
    "#                 result[\"province_match\"] = predicted_provinces[0]\n",
    "\n",
    "#     # Check district\n",
    "#     if expected.get(\"district\"):\n",
    "#         predicted_districts = predicted.get(\"district\", {}).get(\"classified\", [])\n",
    "#         if predicted_districts and predicted_districts[0]:\n",
    "#             predicted_district = predicted_districts[0].lower().strip()\n",
    "#             expected_district = expected[\"district\"].lower().strip()\n",
    "#             if (\n",
    "#                 predicted_district == expected_district\n",
    "#                 or expected_district in predicted_district\n",
    "#                 or predicted_district in expected_district\n",
    "#             ):\n",
    "#                 result[\"district_correct\"] = True\n",
    "#                 result[\"district_match\"] = predicted_districts[0]\n",
    "\n",
    "#     # Check ward\n",
    "#     if expected.get(\"ward\"):\n",
    "#         predicted_wards = predicted.get(\"ward\", {}).get(\"classified\", [])\n",
    "#         if predicted_wards and predicted_wards[0]:\n",
    "#             predicted_ward = predicted_wards[0].lower().strip()\n",
    "#             expected_ward = expected[\"ward\"].lower().strip()\n",
    "#             if (\n",
    "#                 predicted_ward == expected_ward\n",
    "#                 or expected_ward in predicted_ward\n",
    "#                 or predicted_ward in expected_ward\n",
    "#             ):\n",
    "#                 result[\"ward_correct\"] = True\n",
    "#                 result[\"ward_match\"] = predicted_wards[0]\n",
    "\n",
    "#     return result\n",
    "\n",
    "\n",
    "# def safe_get_first(data_dict: Dict, component: str) -> str:\n",
    "#     \"\"\"Safely get the first classified item or return None\"\"\"\n",
    "#     classified = data_dict.get(component, {}).get(\"classified\", [])\n",
    "#     return classified[0] if classified and classified[0] else None\n",
    "\n",
    "\n",
    "# def run_comprehensive_test(\n",
    "#     test_file_path: str = \"test.json\", max_display: int = 10\n",
    "# ) -> Dict:\n",
    "#     \"\"\"\n",
    "#     Run comprehensive test suite matching main.py pattern with timing and export\n",
    "#     \"\"\"\n",
    "#     # Load test cases\n",
    "#     test_cases = load_test_cases(test_file_path)\n",
    "#     total_cases = len(test_cases)\n",
    "\n",
    "#     if total_cases == 0:\n",
    "#         return {\"error\": \"No test cases found\"}\n",
    "\n",
    "#     print(f\"\\nTesting address classification with {total_cases} test cases:\")\n",
    "#     print(\"=\" * 80)\n",
    "\n",
    "#     # Initialize tracking variables\n",
    "#     correct_predictions = 0\n",
    "#     total_time = 0\n",
    "#     max_time = 0\n",
    "#     failed_cases = []\n",
    "#     all_test_results = []\n",
    "\n",
    "#     # Process test cases\n",
    "#     for i, test_case in enumerate(test_cases):\n",
    "#         address = test_case[\"text\"]\n",
    "#         expected = test_case[\"result\"]\n",
    "#         notes = test_case.get(\"notes\", \"\")\n",
    "\n",
    "#         # Time the prediction\n",
    "#         start_time = time.perf_counter()\n",
    "#         normalized_address = normalize_input(address)\n",
    "#         suggestion = suggest_address_components(normalized_address)\n",
    "#         predicted = classify_address_components(\n",
    "#             suggestion, province_trie, district_trie, ward_trie\n",
    "#         )\n",
    "#         end_time = time.perf_counter()\n",
    "\n",
    "#         processing_time = (end_time - start_time) * 1000  # Convert to ms\n",
    "#         total_time += processing_time\n",
    "#         max_time = max(max_time, processing_time)\n",
    "\n",
    "#         # Evaluate prediction\n",
    "#         evaluation = evaluate_prediction(predicted, expected)\n",
    "\n",
    "#         # Check if prediction matches expected result (all components correct)\n",
    "#         is_correct = (\n",
    "#             evaluation[\"province_correct\"]\n",
    "#             and evaluation[\"district_correct\"]\n",
    "#             and evaluation[\"ward_correct\"]\n",
    "#         )\n",
    "\n",
    "#         if is_correct:\n",
    "#             correct_predictions += 1\n",
    "\n",
    "#         # Prepare result data matching main.py format\n",
    "#         reasons = []\n",
    "#         pred_province = safe_get_first(predicted, \"province\")\n",
    "#         pred_district = safe_get_first(predicted, \"district\")\n",
    "#         pred_ward = safe_get_first(predicted, \"ward\")\n",
    "\n",
    "#         if not evaluation[\"province_correct\"]:\n",
    "#             reasons.append(\"province mismatch\")\n",
    "#         if not evaluation[\"district_correct\"]:\n",
    "#             reasons.append(\"district mismatch\")\n",
    "#         if not evaluation[\"ward_correct\"]:\n",
    "#             reasons.append(\"ward mismatch\")\n",
    "#         if processing_time > 100:\n",
    "#             reasons.append(f\"time_exceeded ({processing_time:.2f} ms > 100 ms)\")\n",
    "\n",
    "#         fail_reason = \"; \".join(reasons) if reasons else \"\"\n",
    "\n",
    "#         test_result = {\n",
    "#             \"index\": i + 1,\n",
    "#             \"input\": address,\n",
    "#             \"normalized\": normalized_address,\n",
    "#             \"notes\": notes,\n",
    "#             \"expected_province\": expected.get(\"province\"),\n",
    "#             \"expected_district\": expected.get(\"district\"),\n",
    "#             \"expected_ward\": expected.get(\"ward\"),\n",
    "#             \"got_province\": pred_province,\n",
    "#             \"got_district\": pred_district,\n",
    "#             \"got_ward\": pred_ward,\n",
    "#             \"time_ms\": round(processing_time, 3),\n",
    "#             \"status\": \"PASS\" if is_correct else \"FAIL\",\n",
    "#             \"fail_reason\": fail_reason,\n",
    "#         }\n",
    "\n",
    "#         all_test_results.append(test_result)\n",
    "\n",
    "#         if not is_correct:\n",
    "#             failed_cases.append(test_result)\n",
    "\n",
    "#         # Show results for first few test cases or if incorrect\n",
    "#         if i < max_display or not is_correct:\n",
    "#             print(f\"\\nTest {i + 1}: {address}\")\n",
    "#             print(f\"Expected: {expected}\")\n",
    "#             print(\n",
    "#                 f\"Predicted: {{'province': '{pred_province}', 'district': '{pred_district}', 'ward': '{pred_ward}'}}\"\n",
    "#             )\n",
    "#             print(f\"Correct: {'✅' if is_correct else '❌'}\")\n",
    "#             print(f\"Processing time: {processing_time:.2f}ms\")\n",
    "#             if notes:\n",
    "#                 print(f\"Notes: {notes}\")\n",
    "\n",
    "#         # Check performance requirements\n",
    "#         if processing_time > 100:  # 0.1s = 100ms\n",
    "#             print(\"⚠️  WARNING: Exceeds maximum time requirement!\")\n",
    "#         elif processing_time > 10:  # 0.01s = 10ms\n",
    "#             print(\"⚠️  WARNING: Exceeds average time requirement!\")\n",
    "\n",
    "#     # Summary statistics\n",
    "#     accuracy = (correct_predictions / total_cases) * 100\n",
    "#     avg_time = total_time / total_cases\n",
    "\n",
    "#     print(\"\\n\" + \"=\" * 80)\n",
    "#     print(\"SUMMARY:\")\n",
    "#     print(f\"Total test cases: {total_cases}\")\n",
    "#     print(f\"Correct predictions: {correct_predictions}\")\n",
    "#     print(f\"Accuracy: {accuracy:.2f}%\")\n",
    "#     print(f\"Average processing time: {avg_time:.2f}ms\")\n",
    "#     print(f\"Maximum processing time: {max_time:.2f}ms\")\n",
    "\n",
    "#     print(\"\\n\" + \"=\" * 80)\n",
    "#     print(\"PERFORMANCE CONSTRAINTS\")\n",
    "#     print(\"=\" * 80)\n",
    "#     print(f\"Average time ≤ 10ms: {'✓' if avg_time <= 10 else '✗'} ({avg_time:.2f} ms)\")\n",
    "#     print(f\"Max time ≤ 100ms: {'✓' if max_time <= 100 else '✗'} ({max_time:.2f} ms)\")\n",
    "\n",
    "#     if avg_time <= 10 and max_time <= 100:\n",
    "#         print(\"✅ All performance requirements met!\")\n",
    "#     else:\n",
    "#         print(\"⚠️  Performance requirements not met!\")\n",
    "\n",
    "#     return {\n",
    "#         \"total_cases\": total_cases,\n",
    "#         \"correct_predictions\": correct_predictions,\n",
    "#         \"accuracy\": accuracy,\n",
    "#         \"avg_time\": avg_time,\n",
    "#         \"max_time\": max_time,\n",
    "#         \"failed_cases\": failed_cases,\n",
    "#         \"all_test_results\": all_test_results,\n",
    "#     }\n",
    "\n",
    "\n",
    "# def export_test_results(test_results: Dict):\n",
    "#     \"\"\"Export test results to CSV and JSON files matching main.py format\"\"\"\n",
    "#     if \"error\" in test_results:\n",
    "#         print(\"Cannot export results - test failed to run\")\n",
    "#         return\n",
    "\n",
    "#     all_test_results = test_results[\"all_test_results\"]\n",
    "#     failed_cases = test_results[\"failed_cases\"]\n",
    "\n",
    "#     print(\"\\n\" + \"=\" * 80)\n",
    "#     print(\"EXPORTING TEST RESULTS\")\n",
    "#     print(\"=\" * 80)\n",
    "\n",
    "#     try:\n",
    "#         # CSV headers matching main.py format\n",
    "#         csv_headers = [\n",
    "#             \"index\",\n",
    "#             \"input\",\n",
    "#             \"normalized\",\n",
    "#             \"notes\",\n",
    "#             \"expected_province\",\n",
    "#             \"expected_district\",\n",
    "#             \"expected_ward\",\n",
    "#             \"got_province\",\n",
    "#             \"got_district\",\n",
    "#             \"got_ward\",\n",
    "#             \"time_ms\",\n",
    "#             \"status\",\n",
    "#             \"fail_reason\",\n",
    "#         ]\n",
    "\n",
    "#         # Create output directory if it doesn't exist\n",
    "#         output_dir = \"output\"\n",
    "#         os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "#         # Export full test report\n",
    "#         test_report_path = os.path.join(output_dir, \"_test_report.csv\")\n",
    "#         with open(test_report_path, \"w\", encoding=\"utf-8-sig\", newline=\"\") as cf:\n",
    "#             writer = csv.DictWriter(cf, fieldnames=csv_headers)\n",
    "#             writer.writeheader()\n",
    "#             for row in all_test_results:\n",
    "#                 writer.writerow(row)\n",
    "#         print(\n",
    "#             f\"Wrote test report CSV: {test_report_path} ({len(all_test_results)} rows)\"\n",
    "#         )\n",
    "\n",
    "#         # Export failed cases only\n",
    "#         failed_cases_path = os.path.join(output_dir, \"_failed_cases.csv\")\n",
    "#         with open(failed_cases_path, \"w\", encoding=\"utf-8-sig\", newline=\"\") as cf:\n",
    "#             writer = csv.DictWriter(cf, fieldnames=csv_headers)\n",
    "#             writer.writeheader()\n",
    "#             for row in failed_cases:\n",
    "#                 writer.writerow(row)\n",
    "#         print(f\"Wrote failed cases CSV: {failed_cases_path} ({len(failed_cases)} rows)\")\n",
    "\n",
    "#         # Export JSON format for full report\n",
    "#         json_report_path = os.path.join(output_dir, \"_test_report.json\")\n",
    "#         with open(json_report_path, \"w\", encoding=\"utf-8\") as jf:\n",
    "#             json.dump(all_test_results, jf, ensure_ascii=False, indent=2)\n",
    "#         print(\n",
    "#             f\"Wrote test report JSON: {json_report_path} ({len(all_test_results)} rows)\"\n",
    "#         )\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(f\"Failed to export test results: {e}\")\n",
    "\n",
    "\n",
    "# def show_failed_cases_summary(test_results: Dict, max_show: int = 10):\n",
    "#     \"\"\"Display detailed information about failed test cases\"\"\"\n",
    "#     if \"error\" in test_results:\n",
    "#         return\n",
    "\n",
    "#     failed_cases = test_results.get(\"failed_cases\", [])\n",
    "\n",
    "#     if not failed_cases:\n",
    "#         print(\"🎉 No failed cases!\")\n",
    "#         return\n",
    "\n",
    "#     print(f\"\\nFAILED CASES SUMMARY (showing first {min(max_show, len(failed_cases))}):\")\n",
    "#     print(\"=\" * 80)\n",
    "\n",
    "#     for i, case in enumerate(failed_cases[:max_show]):\n",
    "#         print(f\"\\n[{case['index']}] {case['input']}\")\n",
    "#         print(\n",
    "#             f\"Expected - P: {case['expected_province'] or 'N/A'}, D: {case['expected_district'] or 'N/A'}, W: {case['expected_ward'] or 'N/A'}\"\n",
    "#         )\n",
    "#         print(\n",
    "#             f\"Got      - P: {case['got_province'] or 'N/A'}, D: {case['got_district'] or 'N/A'}, W: {case['got_ward'] or 'N/A'}\"\n",
    "#         )\n",
    "#         print(f\"Time: {case['time_ms']}ms | Issues: {case['fail_reason']}\")\n",
    "\n",
    "#     if len(failed_cases) > max_show:\n",
    "#         print(f\"\\n... and {len(failed_cases) - max_show} more failed cases\")\n",
    "\n",
    "\n",
    "# # Run the comprehensive test suite\n",
    "# print(\"Building classification tries...\")\n",
    "# province_trie, district_trie, ward_trie = build_classification_tries()\n",
    "\n",
    "# print(\"\\nRunning comprehensive test suite...\")\n",
    "# test_results = run_comprehensive_test(\"test.json\", max_display=5)\n",
    "\n",
    "# if \"error\" not in test_results:\n",
    "#     show_failed_cases_summary(test_results, max_show=10)\n",
    "#     export_test_results(test_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
